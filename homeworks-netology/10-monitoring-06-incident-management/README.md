# Ответы на задания 10-monitoring-06-incident-management    

## Постмортем

| Хронология | Описание события  |
|:---|:---|
| **Предпосылки** | В компании организована распределенная по разным регионам схема ЦОД. Плановые работы по замене высокоскоростного сетевого оборудования привели к потере связи между двумя из них. Связь восстановилась за 43 секунды, но вызвала цепочку событий, приведших к ухудшению обслуживания. Для управления топологией кластера используется Оркестратор, который может быть настроен на реализацию топологий, не поддерживаемых приложением |
| **Краткое описание инцидента** | 21 октября 2018 года, около 23:00 случился сбой баз данных в результате чего фиксировалось появление непоследовательной информации на сайте [https://github.com/](https://github.com/)  |
| **Что было предпринято** | Для обеспечения целостности данных пользователей были приостановлены работа событий (hooks) и других внутренних систем обработки|
| **Причина инцидента** | Ошибка репликации из-за разрыва сетевого соединения |
| **Воздействие инцидента** | Записи/комментарии пользователей визуально отображаются, а после перезагрузки — пропадают. Сервисы отображали пользователям устаревшую и непоследовательную информацию |
| **Обнаружение** | Система мониторинга зафиксировала генерацию большого количества уведомлений (alerts), свидетельствующих о нарушениях в работе кластеров баз данных |
| **Реакция** | После обнаружения инцидента было опубликовано уведомление для пользователей, размещенное на странице [https://www.githubstatus.com/messages](https://www.githubstatus.com/messages), на котором в реальном времени можно было видеть статус и приблизительное время до разрешения проблемы. Инженеры группы реагирования вручную заблокировали инструменты развертывания, чтобы недопустить внесения каких-либо дополнительных изменеий, сайт переведен в "желтый" статус, таким образом ситуация перешла в статус активного инцедента. Был привлечен соответствующий специалист - координатор инцидента, который через две минуты перевел статус в "красный". Становится понятно, что ситуация затронула несколько кластеров БД. Инженерами принято решение и начата подготовка к ручной настройке БД, расположенной в ЦОД на Восточном побережье, как основной. Сложность задачи заключалась в том, что сервер БД, расположенный в ЦОД на Западном побережье уже продолжительное время принимал записи. После возникновения понимания, что сервера в ЦОД на Западном побережье не могут справиться с дополнительной задержкой, ради сохранения данных в ущерб доступности было принято решение ухудшить качество обслуживания для достижения согласованности данных. Состояние кластера показывало, что необходимо снизить нагрузку. Для этого push-уведомления были отключены - сохранность данных в приоритете перед удобством использования платформы. Был разработан дальнейший план, который предполагал восстановление данных из резервной копии, синхронизацию реплик, возвращение к стабильной работе, продолжение обработки заданий в очереди. Данные из резервной копии (хранится несколько лет) восстанавливались несколько часов. После восстановления из резервных копий все первичные БД оказались снова установлены в ЦОД на Восточном побережье сайт стал гораздо более отзывчивым на запросы, однако сохранялись отложенные реплики, которые отставали от основной на несколько часов, из-за чего пользователи и наблюдали разрозненные данные. Из-за того, что к этому моменту в США и Европе начался рабочий день и к сайту возрасли обращения количество времени, необходимое на полную репликацию увеличилось. После синхронизации всех реплик был осуществлен переход на исходную топологию, начата обработка накопившихся в очереди данных. Было зарегистрировано более пяти миллионов hook event и поставлено в очередь 80 тысяч сборок страниц. Degraded статус был сохранен до завершения обработки всего объема накопившихся данных|
| **Восстановление** | Через 24 часа и 11 минут работа сервиса была полностью восстановлена  |
| **Последующие действия** | Проведен анализ журналов MySQL на предмет нереплицированных данных; изменен механизм отчетности о статусе работы платформы вцелом и работе конкретных сервисов платформы в частности; проведены настройки Оркестратора, позволяющие предотвратить распространение репликации баз данных за пределы региона; определена цель - выдержать полных отказ одного из центров обработки данных |  
| **Выводы** | Необходимо развивать коммуникации и быть готовыми к незамедлительному информированию пользователей об инцедентах, более тщательно подходить к временным оценкам, чтобы избежать путаницы у пользователей. Необходимо занять более активную позицию по поводу проверки собтвенных гипотез, т.к. с ростом компании увеличивается и ее технологическая сложность; будет реализована систематическая практика проверки сценариев сбоев до того, как они смогут повлиять на пользователей. |  

## Timeline по инциденту

![TIMELINE](assets/timeline.jpg)  


